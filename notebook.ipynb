{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speculative Decoding: Train and Benchmark Medusa\n",
    "\n",
    "Large Language Models (LLMs) are changing our world. However, productionizing them can be slow and expensive. Speculative decoding is a technique that can speed up LLM inference by predicting multiple future tokens in parallel. This can reduce the time required for generating text outputs. However, speculative decoding can be complex to implement. Medusa is a framework that simplifies the speculative decoding process while maintaining its benefits.\n",
    "\n",
    "Medusa accelerates LLM text generation by adding multiple decoding heads to predict several subsequent tokens in parallel, instead of just the next token. It then uses tree attention to efficiently process multiple token candidates simultaneously and a typical acceptance scheme to select plausible continuations, resulting in about a 2x speedup in generation time. By integrating additional \"Medusa heads\" with the original model, it allows for efficient token generation without the need for a separate draft model. \n",
    "\n",
    "This blog post shows you how to train and benchmark Medusa. \n",
    "\n",
    "## Training Medusa\n",
    "\n",
    "Before training our Medusa we need to better understand our data distribution. One of the most important things is to have a good dataset (with similar distribution to what will be used in production) because Medusa has a much higher hit-rate when the generation is in-domain. \n",
    "\n",
    "This means if you are going to train Medusa on a dataset that is very different from the data/user queries you have in production, your speedup will be minimal or non-existent. \n",
    "\n",
    "There are 3 different ways to select/prepare data for training Medusa:\n",
    "\n",
    "1. **Self-distillation**: This is the easiest and most effective way to prepare data for training. You can use the same model to generate the data that you will use to train the model. Essentially, you prompt the model with a similar input to what you will use in production and the model will generate the output.\n",
    "2. **User/Application data**: If you are able to collect real user queries and model outputs, you can use this data to train Medusa. \n",
    "3. **Fine-tuning data**: If you don't have access to user data, you can use the fine-tuning dataset to train Medusa.\n",
    "\n",
    "In this blog post, we will use the fine-tuning data to train Medusa. \n",
    "\n",
    "The dataset or data distribution also plays a key role when evaluating/benchmarking the performance of the Medusa heads. As we learned that Medusa has a much higher hit-rate when the generation is in-domain, it is important to evaluate the Medusa heads on the same data distribution that will be used in production or training. I\n",
    "\n",
    "Okay lets get started. ğŸš€ We will use a smalle modified the [original implementation of Medusa](https://github.com/FasterDecoding/Medusa). The repository includes a training script along side a python package. Lets clone the repository and install the package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Pytorch, Deepspeed & Hugging Face libraries\n",
    "%pip install \"torch==2.4.0\" tensorboard \"transformers==4.44.2\" \"datasets==2.21.0\" \"accelerate==0.33.0\" \"deepspeed==0.14.5\"\n",
    "# Download and install Medusa packages\n",
    "# !git clone https://github.com/philschmid/Medusa\n",
    "# !cd Medusa && pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use the same SFT dataset as used for the model. As model we will use [philschmid/code-llama-3-1-8b-text-to-sql](https://huggingface.co/philschmid/code-llama-3-1-8b-text-to-sql) a Llama 3.1 8b Q-Lora fine-tuned LLM for text-to-sql task. The SFT dataset is available at [philschmid/text-to-sql-dataset-medusa](https://huggingface.co/datasets/philschmid/text-to-sql-dataset-medusa). \n",
    "\n",
    "The dataset includes 10,000 training samples, which where used for SFT and 2,500 test samples. We will later use those unseen 2,500 test samples for benchmarking our edusa Model.\n",
    "\n",
    "First Lets download our dataset and save it to a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import json\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"philschmid/text-to-sql-dataset-medusa\")\n",
    "\n",
    "# Save the train dataset as list to disk as JSON\n",
    "with open(\"train_dataset.json\", \"w\") as f:\n",
    "    json.dump(list(dataset[\"train\"][\"messages\"]), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready to train our Medusa model. For Medusa there are 2 important hyperparameters:\n",
    "\n",
    "* `medusa_heads`: controls the number of additional decoding heads added to the language model. These heads predict multiple future tokens in parallel, speeding up inference. \n",
    "* `medusa_layers`: The number of layers to use for each Medusa heads. \n",
    "\n",
    "We can start our training using the existing `train_legacy.py` script and provide our parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!torchrun --nproc_per_node=4  Medusa/medusa/train/train_legacy.py \\\n",
    "    --model_name_or_path philschmid/code-llama-3-1-8b-text-to-sql \\\n",
    "    --data_path train_dataset.json \\\n",
    "    --bf16 True \\\n",
    "    --output_dir code_llama31 \\\n",
    "    --num_train_epochs 5 \\\n",
    "    --per_device_train_batch_size 1 \\\n",
    "    --gradient_accumulation_steps 4 \\\n",
    "    --eval_strategy \"no\" \\\n",
    "    --save_strategy \"epoch\" \\\n",
    "    --learning_rate 5e-4 \\\n",
    "    --lr_scheduler_type \"cosine\" \\\n",
    "    --warmup_steps 40 \\\n",
    "    --logging_steps 10 \\\n",
    "    --tf32 True \\\n",
    "    --model_max_length 2048 \\\n",
    "    --lazy_preprocess False \\\n",
    "    --medusa_num_heads 3 \\\n",
    "    --medusa_num_layers 1 \\\n",
    "    --deepspeed ./Medusa/deepspeed.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice our training successfully finished. Now we just need to push our model to the hub and we are ready to benchmark our model. Make sure to replace the `folder` and `repo` with your own values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m medusa.hf_utils \\\n",
    "    --folder code_llama31_medusa \\\n",
    "    --repo philschmid/code-llama-3-1-8b-text-to-sql-medusa "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Medusa with Hugging Face Text Generation Inference \n",
    "\n",
    "TODO: Text about what TGI is on what it supports \n",
    "\n",
    "When using Speculative decoding we want to measure the acceleration of our model through the Medusa heads. \"Acceleration\" refers to the speedup achieved during speculative decoding. Specifically, acceleration is calculated as the ratio of the total number of tokens (both generated and skipped) to the number of iterations or loops needed to produce those tokens.\n",
    "\n",
    "Acceleration measures how much faster a model can produce output when it speculates multiple tokens ahead. The formula given is.  \n",
    "`Acceleration = (Total Tokens (Generated + Skipped)) / (Number of Loops/Iterations)`\n",
    "\n",
    "acceleration = (17687 + 27101) / 27101 = 1.65\n",
    "\n",
    "\n",
    "\n",
    "Lets start our TGI container using the `docker run` command.\n",
    "```bash\n",
    "CUDA_VISIBLE_DEVICES=0 docker run --gpus all -ti --shm-size 1g --ipc=host --rm -p 8080:80 \\\n",
    "  -e MODEL_ID=philschmid/code-llama-3-1-8b-text-to-sql-medusa \\\n",
    "  -e NUM_SHARD=1 \\\n",
    "  -e MAX_INPUT_TOKENS=4096 \\\n",
    "  -e MAX_TOTAL_TOKENS=6000 \\\n",
    "  ghcr.io/huggingface/text-generation-inference:sha-b70ae09\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After our container is running, we can test it with a simple query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "curl localhost:8080/v1/chat/completions \\\n",
    "    -X POST \\\n",
    "    -d '{\n",
    "  \"model\": \"tgi\",\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Write a poem for my three year old\"\n",
    "    }\n",
    "  ],\n",
    "  \"stream\": false,\n",
    "  \"max_tokens\": 250\n",
    "}' \\\n",
    "    -H 'Content-Type: application/json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Benchmarking we will use guidellm from NeuralMagic. GuideLLM can be used to simulate real-world inference workloads, GuideLLM helps users gauge the performance, resource needs, and cost implications of deploying LLMs on various hardware configurations. Supporting Hugigng Face dataset from local files or remote for benchmarking. If you are planning to use a Hugging Face dataset, you need to make sure that the dataset includes a `text` field, with the formatted prompt (system + user). \n",
    "\n",
    "Here is a simple example on how to create on. (It was used to create the test set):\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"philschmid/text-to-sql-dataset-medusa\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"philschmid/code-llama-3-1-8b-text-to-sql\")\n",
    "\n",
    "def create_text_field(samples):\n",
    "    prompt = tokenizer.apply_chat_template(samples[\"messages\"][0:2], tokenize=False)\n",
    "    return {\"text\": prompt}\n",
    "  \n",
    "td = dataset[\"test\"].map(create_text_field)\n",
    "td.push_to_hub(\"philschmid/text-to-sql-dataset-medusa-test-chatml\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install guidellm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GuideLLM will use the OpenAI API and run through our test dataset 2,500 queries with different concurrent requests. \n",
    "\n",
    "```bash\n",
    "guidellm \\\n",
    "  --target \"http://localhost:8080/v1\" \\\n",
    "  --model philschmid/code-llama-3-1-8b-text-to-sql-medusa \\\n",
    "  --data philschmid/text-to-sql-dataset-medusa-test-chatml \\\n",
    "  --data-type transformers \\\n",
    "  --max-seconds 60 \\\n",
    "  --output-path benchmark_medusa.json\n",
    "```\n",
    "\n",
    "Running Llama 3.1 8b with Medusa heads on a single GPU we get the following results: \n",
    "\n",
    "```                                                                                                     \n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“ \n",
    "â”ƒ Benchmark                 â”ƒ Requests per Second â”ƒ Request Latency â”ƒ Time to First Token â”ƒ Inter Token Latency â”ƒ Output Token Throughput â”ƒ \n",
    "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”© \n",
    "â”‚ synchronous               â”‚ 0.86 req/sec        â”‚ 1.16 sec        â”‚ 80.81 ms            â”‚ 50.32 ms            â”‚ 18.53 tokens/sec        â”‚ \n",
    "â”‚ throughput                â”‚ 3.91 req/sec        â”‚ 12.48 sec       â”‚ 10824.93 ms         â”‚ 77.18 ms            â”‚ 84.42 tokens/sec        â”‚ \n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  \n",
    "```\n",
    "\n",
    "Now, lets compare this to our base model without Medusa. There for we use the same `docker run` command but replace our Model id. \n",
    "\n",
    "```Bash\n",
    "CUDA_VISIBLE_DEVICES=0 docker run --gpus all -ti --shm-size 1g --ipc=host --rm -p 8080:80 \\\n",
    "  -e MODEL_ID=philschmid/code-llama-3-1-8b-text-to-sql \\\n",
    "  -e NUM_SHARD=1 \\\n",
    "  -e MAX_INPUT_TOKENS=4096 \\\n",
    "  -e MAX_TOTAL_TOKENS=6000 \\\n",
    "  ghcr.io/huggingface/text-generation-inference:sha-b70ae09\n",
    "```\n",
    "\n",
    "And then rerun our benchmark, make sure to change the output path to `benchmark_baseline.json`. \n",
    "\n",
    "```\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“ \n",
    "â”ƒ Benchmark                 â”ƒ Requests per Second â”ƒ Request Latency â”ƒ Time to First Token â”ƒ Inter Token Latency â”ƒ Output Token Throughput â”ƒ \n",
    "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”© \n",
    "â”‚ synchronous               â”‚ 0.71 req/sec        â”‚ 1.40 sec        â”‚ 81.08 ms            â”‚ 60.50 ms            â”‚ 15.56 tokens/sec        â”‚ \n",
    "â”‚ throughput                â”‚ 3.76 req/sec        â”‚ 13.11 sec       â”‚ 11370.37 ms         â”‚ 81.92 ms            â”‚ 79.66 tokens/sec        â”‚ \n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

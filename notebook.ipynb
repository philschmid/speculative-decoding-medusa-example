{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speculative Decoding: Train and Benchmark Medusa\n",
    "\n",
    "Large Language Models (LLMs) are changing our world. However, productionizing them can be slow and expensive. Speculative decoding is a technique that can speed up LLM inference by predicting multiple future tokens in parallel. This can reduce the time required for generating text outputs. However, speculative decoding can be complex to implement. Medusa is a framework that simplifies the speculative decoding process while maintaining its benefits.\n",
    "\n",
    "Medusa accelerates LLM text generation by adding multiple decoding heads to predict several subsequent tokens in parallel, instead of just the next token. It then uses tree attention to efficiently process multiple token candidates simultaneously and a typical acceptance scheme to select plausible continuations, resulting in about a 2x speedup in generation time. By integrating additional \"Medusa heads\" with the original model, it allows for efficient token generation without the need for a separate draft model. \n",
    "\n",
    "This blog post shows you how to train and benchmark Medusa. \n",
    "\n",
    "_Note: This examples was run on aws `g6e.12xlarge` with 4x NVIDIA L40S GPUs with each 48GB Memory._\n",
    "\n",
    "## Training Medusa\n",
    "\n",
    "Before training our Medusa we need to better understand our data distribution. One of the most important things is to have a good dataset (with similar distribution to what will be used in production) because Medusa has a much higher hit-rate when the generation is in-domain. \n",
    "\n",
    "This means if you are going to train Medusa on a dataset that is very different from the data/user queries you have in production, your speedup will be minimal or non-existent. \n",
    "\n",
    "There are 3 different ways to select/prepare data for training Medusa:\n",
    "\n",
    "1. **Self-distillation**: This is the easiest and most effective way to prepare data for training. You can use the same model to generate the data that you will use to train the model. Essentially, you prompt the model with a similar input to what you will use in production and the model will generate the output.\n",
    "2. **User/Application data**: If you are able to collect real user queries and model outputs, you can use this data to train Medusa. \n",
    "3. **Fine-tuning data**: If you don't have access to user data, you can use the fine-tuning dataset to train Medusa.\n",
    "\n",
    "In this blog post, we will use the Self-distillation data to train Medusa. \n",
    "\n",
    "The dataset or data distribution also plays a key role when evaluating/benchmarking the performance of the Medusa heads. As we learned that Medusa has a much higher hit-rate when the generation is in-domain, it is important to evaluate the Medusa heads on the same data distribution that will be used in production or training. I\n",
    "\n",
    "Okay lets get started. ðŸš€ We will use a smalle modified the [original implementation of Medusa](https://github.com/FasterDecoding/Medusa). The repository includes a training script along side a python package. Lets clone the repository and install the package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Pytorch, Deepspeed & Hugging Face libraries\n",
    "%pip install \"torch==2.4.0\" tensorboard \"transformers==4.44.2\" \"datasets==2.21.0\" \"accelerate==0.33.0\" \"deepspeed==0.14.5\"\n",
    "# Download and install Medusa packages\n",
    "# !git clone https://github.com/philschmid/Medusa\n",
    "# !cd Medusa && pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Self-distillation data\n",
    "\n",
    "To create the self-distillation data we will use the same model that we will use to train Medusa. In this blog post we will use [meta-llama/Meta-Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct) and prompts from the Open-Orca dataset. We use Hugging Face TGI to start an endpoint and asynchronously iterate over the dataset to generate our self-distilled responses. We are going to create 15,000 examples for training Medusa.\n",
    "\n",
    "```bash\n",
    "docker run --gpus all -ti --shm-size 1g --ipc=host --rm -p 8080:80 \\\n",
    "  -e MODEL_ID=meta-llama/Meta-Llama-3.1-8B-Instruct \\\n",
    "  -e NUM_SHARD=4 \\\n",
    "  -e MAX_INPUT_TOKENS=7168 \\\n",
    "  -e MAX_TOTAL_TOKENS=8192 \\\n",
    "  -e HF_TOKEN=$(cat ~/.cache/huggingface/token) \\\n",
    "  ghcr.io/huggingface/text-generation-inference:sha-b70ae09\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json \n",
    "from openai import AsyncOpenAI\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "from datasets import load_dataset\n",
    "\n",
    "# max concurrency\n",
    "sem = asyncio.Semaphore(64)\n",
    "\n",
    "# Initialize the client using the Hugging Face Inference API\n",
    "client = AsyncOpenAI(\n",
    "    base_url=\"http://localhost:8080/v1\",\n",
    "    api_key=\"-\",\n",
    ")\n",
    "\n",
    "# Generate Response for input message\n",
    "async def gen_response(sample):\n",
    "    # Comment in if you want to see the prompt\n",
    "    try:\n",
    "        conv = sample[\"messages\"][:-1]\n",
    "        response = await client.chat.completions.create(\n",
    "            model=\"tgi\",\n",
    "            messages=conv,\n",
    "            temperature=0,\n",
    "            max_tokens=1024,\n",
    "        )\n",
    "        results = response.choices[0].message.content\n",
    "        conv.append({\"role\": \"assistant\", \"content\": results})\n",
    "        # Add the evaluation results to the sample\n",
    "        return conv\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "# Combined async helper method to handle concurrent scoring and\n",
    "async def gen_data(dataset):\n",
    "    async def _gen(sample):\n",
    "        async with sem:\n",
    "            res = await gen_response(sample)\n",
    "            progress_bar.update(1)\n",
    "            return res\n",
    "\n",
    "    progress_bar = tqdm_asyncio(total=len(dataset), desc=\"Generating self-distillation\", unit=\"sample\")\n",
    "    tasks = [_gen(text) for text in dataset]\n",
    "    responses = await tqdm_asyncio.gather(*tasks)\n",
    "    progress_bar.close()\n",
    "    # filter out the None responses\n",
    "    responses = [r for r in responses if r is not None]\n",
    "    # save the responses to a file\n",
    "    with open(f\"self_distil_train.json\", \"w\") as f:\n",
    "        json.dump(responses, f)\n",
    "    return responses\n",
    "\n",
    "# Load the dataset and select a 15,000k subset\n",
    "dataset = load_dataset(\"philschmid/slimorca-deduped-cleaned-corrected-chatml\")\n",
    "sub_data = dataset[\"train\"].shuffle(seed=42).select(range(15000))\n",
    "\n",
    "# generate the responses\n",
    "await gen_data(sub_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train speculative Medusa Heads\n",
    "\n",
    "Now, we are ready to train our Medusa model. For Medusa there are 2 important hyperparameters:\n",
    "\n",
    "* `medusa_heads`: controls the number of additional decoding heads added to the language model. These heads predict multiple future tokens in parallel, speeding up inference. \n",
    "* `medusa_layers`: The number of layers to use for each Medusa heads. \n",
    "\n",
    "We can start our training using the existing `train_legacy.py` script and provide our parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!torchrun --nproc_per_node=4  Medusa/medusa/train/train_legacy.py \\\n",
    "    --model_name_or_path meta-llama/Meta-Llama-3.1-8B-Instruct \\\n",
    "    --data_path self_distil_train.json \\\n",
    "    --bf16 True \\\n",
    "    --output_dir llama31_instruct \\\n",
    "    --num_train_epochs 5 \\\n",
    "    --per_device_train_batch_size 4 \\\n",
    "    --gradient_accumulation_steps 4 \\\n",
    "    --eval_strategy \"no\" \\\n",
    "    --save_strategy \"epoch\" \\\n",
    "    --learning_rate 1e-3 \\\n",
    "    --lr_scheduler_type \"cosine\" \\\n",
    "    --warmup_steps 40 \\\n",
    "    --logging_steps 10 \\\n",
    "    --tf32 True \\\n",
    "    --model_max_length 2048 \\\n",
    "    --lazy_preprocess False \\\n",
    "    --medusa_num_heads 3 \\\n",
    "    --medusa_num_layers 1 \\\n",
    "    --deepspeed ./Medusa/deepspeed.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice our training successfully finished. Now we just need to push our model to the hub and we are ready to benchmark our model. Make sure to replace the `folder` and `repo` with your own values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the checkpoints for faster upload\n",
    "!rm -rf llama31_instruct_medusa/checkpoint*\n",
    "# push model to the hub\n",
    "!huggingface-cli upload philschmid/llama-3-1-8b-instruct-medusa llama31_instruct_medusa --repo-type=model  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Medusa with Hugging Face Text Generation Inference \n",
    "\n",
    "TODO: Text about what TGI is on what it supports \n",
    "\n",
    "When using Speculative decoding we want to measure the acceleration of our model through the Medusa heads. \"Acceleration\" refers to the speedup achieved during speculative decoding. Specifically, acceleration is calculated as the ratio of the total number of tokens (both generated and skipped) to the number of iterations or loops needed to produce those tokens.\n",
    "\n",
    "Acceleration measures how much faster a model can produce output when it speculates multiple tokens ahead. The formula given is.  \n",
    "`Acceleration = (Total Tokens (Generated + Skipped)) / (Number of Loops/Iterations)`\n",
    "\n",
    "acceleration = (17687 + 27101) / 27101 = 1.65\n",
    "\n",
    "Lets start our TGI container using the `docker run` command. Make sure to replace the `MODEL_ID` with your own model.\n",
    "```bash\n",
    "CUDA_VISIBLE_DEVICES=0 docker run --gpus all -ti --shm-size 1g --ipc=host --rm -p 8080:80 \\\n",
    "  -e MODEL_ID=meta-llama/Meta-Llama-3.1-8B-Instruct \\\n",
    "  -e NUM_SHARD=1 \\\n",
    "  -e MAX_INPUT_TOKENS=10000 \\\n",
    "  -e MAX_TOTAL_TOKENS=12444 \\\n",
    "  -e HF_TOKEN=$(cat ~/.cache/huggingface/token) \\\n",
    "  ghcr.io/huggingface/text-generation-inference:sha-b70ae09\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After our container is running, we can test it with a simple query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "349880da0d3647c488903dece0f15650",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/267893 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset \n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "dataset = load_dataset (\"Open-Orca/OpenOrca\")[\"train\"]\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\n",
    "\n",
    "seq_len = lambda x: len (tokenizer.encode(x[\"question\"], add_special_tokens=False))\n",
    "dataset_filtered = dataset.filter(lambda x: 500 < seq_len (x) < 1000, num_proc=32)\n",
    "\n",
    "def create_conversation(x):\n",
    "    messages = []\n",
    "    if x[\"system_prompt\"] is not None:\n",
    "        messages.append({\"role\": \"system\", \"content\": x[\"system_prompt\"]})\n",
    "    else:\n",
    "        messages.append({\"role\": \"system\", \"content\": \"You are a helpful assistant.\"})\n",
    "    \n",
    "    messages.append({\"role\": \"user\", \"content\": x[\"question\"]})\n",
    "  \n",
    "    return {\"messages\": messages}\n",
    "\n",
    "dataset_subset = dataset_filtered.map(create_conversation)\n",
    "dataset_subset = dataset_subset.shuffle(42).select(range(10000))\n",
    "dataset_subset = dataset_subset.rename_column(\"question\", \"text\")\n",
    "dataset_subset = dataset_subset.remove_columns([\"system_prompt\", \"response\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 't0.216446',\n",
       " 'text': 'Read the following article and answer the question. Article: \"I say, I\\'m pleased to see you,\" said the little man standing by the letter-box. \"Oh, hello,\" I said, remembering he was a new neighbor. \"Simpson, isn\\'t it?\" \"Yes, that\\'s right.\" He seemed quite pleased by my ready recognition. \"I wonder if you could lend me some money,\" he continued. \"My wife gave me a letter to post, and I\\'ve just noticed it isn\\'t stamped.\" \"yes, they never are,\" I said, sympathetically . \"It must go tonight--it really must! I\\'d get stamps out of the machine,\" explained Simpson,\" Only I find I have no small change about me.\" \"I\\'m sorry, but I\\'m afraid I haven\\'t either,\" I said. \"Oh, dear, dear,\" he said. \"Yes, well,\" I said, intending to move off. But he looked so unhappy standing there with the blue unstamped envelope that I really hadn\\'t the heart to desert him. So I took him to my house and found some pennies and gave them to him, who, in the most business like way, made a note of the loan in his pocket-book, and left. But soon he turned up again. \"I\\'m sorry I am a stranger round here and --well, I\\'m rather lost...\" It took me several minutes to explain to him where the post office was. In the end I felt as lost as Simpson and had to accompany him to the post office, but, only to find the automatic stamp-machine was empty! \"Oh!\" Simpson was so desperate that he dropped the letter on the ground and when he picked it up there was a large black spot on its face. \"Dear me,\" he said, \"My wife told me to post it tonight. I\\'d better post it, if you know what I mean.\" I did know. Or, at least, I knew Mrs Simpson. Then I got a good idea, \"Post the letter unstamped--let the other man pay double postage on it in the morning. \" And he had to agree. Finishing off our job, I took him home. \"I\\'m so grateful to you, really,\" he said when we reached his home. \"That letter--it\\'s only an invitation to dinner to Mr... Dear me!\" \"Why, what\\'s the matter?\" \"Nothing. Just something I\\'ve remembered.\" \"What?\" But he didn\\'t tell me. He just opened his eyes and his mouth at me like a wounded gold- fish, murmured a... Question: In the writer\\'s view,     _   . Answer:\\nAnswer:',\n",
       " 'messages': [{'content': 'You are an AI assistant that follows instruction extremely well. Help as much as you can.',\n",
       "   'role': 'system'},\n",
       "  {'content': 'Read the following article and answer the question. Article: \"I say, I\\'m pleased to see you,\" said the little man standing by the letter-box. \"Oh, hello,\" I said, remembering he was a new neighbor. \"Simpson, isn\\'t it?\" \"Yes, that\\'s right.\" He seemed quite pleased by my ready recognition. \"I wonder if you could lend me some money,\" he continued. \"My wife gave me a letter to post, and I\\'ve just noticed it isn\\'t stamped.\" \"yes, they never are,\" I said, sympathetically . \"It must go tonight--it really must! I\\'d get stamps out of the machine,\" explained Simpson,\" Only I find I have no small change about me.\" \"I\\'m sorry, but I\\'m afraid I haven\\'t either,\" I said. \"Oh, dear, dear,\" he said. \"Yes, well,\" I said, intending to move off. But he looked so unhappy standing there with the blue unstamped envelope that I really hadn\\'t the heart to desert him. So I took him to my house and found some pennies and gave them to him, who, in the most business like way, made a note of the loan in his pocket-book, and left. But soon he turned up again. \"I\\'m sorry I am a stranger round here and --well, I\\'m rather lost...\" It took me several minutes to explain to him where the post office was. In the end I felt as lost as Simpson and had to accompany him to the post office, but, only to find the automatic stamp-machine was empty! \"Oh!\" Simpson was so desperate that he dropped the letter on the ground and when he picked it up there was a large black spot on its face. \"Dear me,\" he said, \"My wife told me to post it tonight. I\\'d better post it, if you know what I mean.\" I did know. Or, at least, I knew Mrs Simpson. Then I got a good idea, \"Post the letter unstamped--let the other man pay double postage on it in the morning. \" And he had to agree. Finishing off our job, I took him home. \"I\\'m so grateful to you, really,\" he said when we reached his home. \"That letter--it\\'s only an invitation to dinner to Mr... Dear me!\" \"Why, what\\'s the matter?\" \"Nothing. Just something I\\'ve remembered.\" \"What?\" But he didn\\'t tell me. He just opened his eyes and his mouth at me like a wounded gold- fish, murmured a... Question: In the writer\\'s view,     _   . Answer:\\nAnswer:',\n",
       "   'role': 'user'}]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_subset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "991fd102954e44b4ac7e8e7e4ecc7858",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80b7ffd565344edaa8f456eaa151b898",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/philschmid/open-orca-10k-guidellm/commit/d950c66ec3fad23c0dc5da9318aad3d9f8b2d20a', commit_message='Upload dataset', commit_description='', oid='d950c66ec3fad23c0dc5da9318aad3d9f8b2d20a', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/philschmid/open-orca-10k-guidellm', endpoint='https://huggingface.co', repo_type='dataset', repo_id='philschmid/open-orca-10k-guidellm'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_subset.push_to_hub(\"philschmid/open-orca-10k-guidellm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curl localhost:8080/v1/completions \\\n",
    "    -X POST \\\n",
    "    -d '{\n",
    "  \"model\": \"tgi\",\n",
    "  \"prompt\": \"hello\",\n",
    "  \"stream\": false,\n",
    "  \"max_tokens\": 250\n",
    "}' \\\n",
    "    -H 'Content-Type: application/json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "curl localhost:8080/v1/chat/completions \\\n",
    "    -X POST \\\n",
    "    -d '{\n",
    "  \"model\": \"tgi\",\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Write a poem for my three year old\"\n",
    "    }\n",
    "  ],\n",
    "  \"stream\": false,\n",
    "  \"max_tokens\": 250\n",
    "}' \\\n",
    "    -H 'Content-Type: application/json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Benchmarking we will use guidellm from NeuralMagic. GuideLLM can be used to simulate real-world inference workloads, GuideLLM helps users gauge the performance, resource needs, and cost implications of deploying LLMs on various hardware configurations. Supporting Hugigng Face dataset from local files or remote for benchmarking. If you are planning to use a Hugging Face dataset, you need to make sure that the dataset includes a `text` field, with the formatted prompt (system + user). \n",
    "\n",
    "Here is a simple example on how to create on. (It was used to create the test set):\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"philschmid/text-to-sql-dataset-medusa\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"philschmid/code-llama-3-1-8b-text-to-sql\")\n",
    "\n",
    "def create_text_field(samples):\n",
    "    prompt = tokenizer.apply_chat_template(samples[\"messages\"][0:2], tokenize=False)\n",
    "    return {\"text\": prompt}\n",
    "  \n",
    "td = dataset[\"test\"].map(create_text_field)\n",
    "td.push_to_hub(\"philschmid/text-to-sql-dataset-medusa-test-chatml\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [12:48<00:00,  3.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. Time to First Token: 0.14 seconds\n",
      "Avg. Inter-Token Latency: 28.75 ms/token\n",
      "Avg. Output Token Throughput: 33.28 tokens/sec\n",
      "Avg. Generated Tokens: 105.40 tokens/sec\n",
      "p50 Time to First Token: 0.09 seconds\n",
      "p50 Inter-Token Latency: 28.59 ms/token\n",
      "p50 Output Token Throughput: 33.43 tokens/sec\n",
      "p50 Generated Tokens: 86.00 tokens/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from statistics import median\n",
    "\n",
    "dataset = load_dataset(\"philschmid/open-orca-250-guidellm\",split=\"train\")\n",
    "# Set up OpenAI API (make sure to use your actual API key)\n",
    "client = OpenAI(api_key=\"-\",base_url=\"http://localhost:8080/v1\")\n",
    "\n",
    "def measure_performance(client, dataset):\n",
    "    start_time = time.time()\n",
    "    successful_requests = 0\n",
    "    results = []\n",
    "    for sample in tqdm(dataset):\n",
    "        has_ttft = False\n",
    "        token_count = 0\n",
    "        start_request = time.time()\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"tgi\",\n",
    "            messages=[{\"role\": \"user\", \"content\": sample[\"text\"]}],\n",
    "            # messages=sample[\"messages\"][0:2],\n",
    "            max_tokens=250,\n",
    "            temperature=0.0,\n",
    "            stream=True\n",
    "        )\n",
    "        start_response = None\n",
    "        start_request2 = time.time()\n",
    "        for chunk in response:\n",
    "            if start_response is None:\n",
    "                start_response = time.time()\n",
    "            if not has_ttft:\n",
    "                ttft = time.time() - start_request\n",
    "                has_ttft = True\n",
    "                # print(f\"Time to First Token: {ttft:.2f} seconds\")\n",
    "            token_count += 1\n",
    "        end_request = time.time()\n",
    "        itl = (end_request - start_response) / (token_count-1) * 1000\n",
    "        # print(f\"Output Token Throughput: {token_count / end_request:.2f} tokens/sec\")\n",
    "        results.append({\"time_to_first_token\": ttft, \"output_token_throughput\": token_count / (end_request - start_request2), \"inter_token_latency\": itl, \"generated_tokens\": token_count})\n",
    "    return results\n",
    "\n",
    "results = measure_performance(client, dataset)\n",
    "\n",
    "print(f\"Avg. Time to First Token: {sum([r['time_to_first_token'] for r in results]) / len(results):.2f} seconds\")\n",
    "print(f\"Avg. Inter-Token Latency: {sum([r['inter_token_latency'] for r in results]) / len(results):.2f} ms/token\")\n",
    "print(f\"Avg. Output Token Throughput: {sum([r['output_token_throughput'] for r in results]) / len(results):.2f} tokens/sec\")\n",
    "print(f\"Avg. Generated Tokens: {sum([r['generated_tokens'] for r in results]) / len(results):.2f} tokens/sec\")\n",
    "\n",
    "# Calculate p50 (median) for each metric\n",
    "p50_ttft = median([r[\"time_to_first_token\"] for r in results])\n",
    "p50_itl = median([r[\"inter_token_latency\"] for r in results if r[\"inter_token_latency\"] is not None])\n",
    "p50_throughput = median([r[\"output_token_throughput\"] for r in results])\n",
    "p50_generated_tokens = median([r[\"generated_tokens\"] for r in results])\n",
    "\n",
    "# Print the p50 values\n",
    "print(f\"p50 Time to First Token: {p50_ttft:.2f} seconds\")\n",
    "print(f\"p50 Inter-Token Latency: {p50_itl:.2f} ms/token\")\n",
    "print(f\"p50 Output Token Throughput: {p50_throughput:.2f} tokens/sec\")\n",
    "print(f\"p50 Generated Tokens: {p50_generated_tokens:.2f} tokens/sec\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "docker run --gpus all --shm-size 1g --ipc=host --rm -p 8080:8000 \\\n",
    "    -e HUGGING_FACE_HUB_TOKEN=$(cat ~/.cache/huggingface/token) \\\n",
    "    vllm/vllm-openai:latest \\\n",
    "    --model mistralai/Mixtral-8x7B-Instruct-v0.1 \\\n",
    "    --tensor-parallel-size 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  -e MODEL_ID=text-generation-inference/Mixtral-8x7B-Instruct-v0.1-medusa \\\n",
    "CUDA_VISIBLE_DEVICES=0 \n",
    "docker run --gpus all -ti --shm-size 1g --ipc=host --rm -p 8080:80 \\\n",
    "  -e MODEL_ID=mistralai/Mixtral-8x7B-Instruct-v0.1 \\\n",
    "  -e NUM_SHARD=8 \\\n",
    "  -e MAX_INPUT_TOKENS=4000 \\\n",
    "  -e MAX_TOTAL_TOKENS=4096 \\\n",
    "  -e HF_TOKEN=$(cat ~/.cache/huggingface/token) \\\n",
    "  ghcr.io/huggingface/text-generation-inference:2.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running:   0%|          | 0/500 [00:00<?, ?sample/s]No chat template is set for this tokenizer, falling back to a default class-level template. This is very error-prone, because models are often trained with templates different from the class default! Default chat templates are a legacy feature and will be removed in Transformers v4.43, at which point any code depending on them will stop working. We recommend setting a valid chat template before then to ensure that this model continues working without issues.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [12:04<00:00,  1.45s/it]1.67s/sample]\n",
      "Running: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [12:04<00:00,  1.45s/sample]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concurrency: 4\n",
      "p50 Time to First Token: 0.07 seconds\n",
      "p50 Inter-Token Latency: 41.96 ms/token\n",
      "p50 Output Token Throughput: 24.03 tokens/second/user\n",
      "p50 Input Tokens: 591.00 tokens\n",
      "p50 Generated Tokens: 120.00 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import time\n",
    "from openai import AsyncOpenAI\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from statistics import median\n",
    "\n",
    "# max concurrency\n",
    "concurrency = 4\n",
    "sem = asyncio.Semaphore(concurrency)\n",
    "\n",
    "# Initialize the client using the Hugging Face Inference API\n",
    "client = AsyncOpenAI(\n",
    "    base_url=\"http://localhost:8080/v1\",\n",
    "    api_key=\"-\",\n",
    ")\n",
    "\n",
    "# Generate Response for input message\n",
    "async def gen_response(sample):\n",
    "    # Comment in if you want to see the prompt\n",
    "    try:\n",
    "        prompt = tokenizer.apply_chat_template(sample[\"messages\"], tokenize=False)\n",
    "        input_tokens = len(tokenizer.encode(prompt, add_special_tokens=False))\n",
    "        has_ttft = False\n",
    "        token_count = 0\n",
    "        start_request = time.time()\n",
    "        response = await client.chat.completions.create(\n",
    "            model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "            messages=sample[\"messages\"],\n",
    "            temperature=0,\n",
    "            max_tokens=250,\n",
    "            stream=True\n",
    "        )\n",
    "        start_response = None\n",
    "        async for chunk in response:\n",
    "            if start_response is None:\n",
    "                start_response = time.time()\n",
    "            if not has_ttft:\n",
    "                ttft = time.time() - start_request\n",
    "                has_ttft = True\n",
    "                # print(f\"Time to First Token: {ttft:.2f} seconds\")\n",
    "            token_count += 1\n",
    "        end_request = time.time()\n",
    "        itl = (end_request - start_response) / (token_count-1) * 1000\n",
    "        throughput = token_count / (end_request - start_response)\n",
    "        # print(f\"Output Token Throughput: {token_count / end_request:.2f} tokens/sec\")\n",
    "        return {\"time_to_first_token\": ttft, \"output_token_throughput\": throughput, \"inter_token_latency\": itl, \"generated_tokens\": token_count, \"input_tokens\": input_tokens}\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "# Combined async helper method to handle concurrent scoring and\n",
    "async def gen_data(dataset):\n",
    "    async def _gen(sample):\n",
    "        async with sem:\n",
    "            res = await gen_response(sample)\n",
    "            progress_bar.update(1)\n",
    "            return res\n",
    "\n",
    "    progress_bar = tqdm_asyncio(total=len(dataset), desc=\"Running\", unit=\"sample\")\n",
    "    tasks = [_gen(text) for text in dataset]\n",
    "    responses = await tqdm_asyncio.gather(*tasks)\n",
    "    progress_bar.close()\n",
    "    return responses\n",
    "\n",
    "# Load the dataset and select a 15,000k subset\n",
    "# dataset = load_dataset(\"philschmid/slimorca-deduped-cleaned-corrected-chatml\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\n",
    "# dataset = load_dataset(\"Aeala/ShareGPT_Vicuna_unfiltered\", split=\"train\")\n",
    "\n",
    "# def get_prompt(x):\n",
    "#     for c in x[\"conversations\"]:\n",
    "#         if c[\"from\"] == \"human\":\n",
    "#             return {\"messages\": [{\"role\": \"user\", \"content\": c[\"value\"]}]}\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "# dataset = dataset.map(get_prompt, remove_columns=[\"conversations\"])\n",
    "# seq_len = lambda x: len(tokenizer.encode(x[\"messages\"][0][\"content\"], add_special_tokens=False))\n",
    "# dataset_filtered = dataset.filter(lambda x: 500 < seq_len(x) < 1000, num_proc=32)\n",
    "# dataset = dataset_filtered.shuffle(seed=42).select(range(500))\n",
    "dataset = load_dataset(\"philschmid/open-orca-10k-guidellm\", split=\"train\")\n",
    "dataset = dataset.select(range(500))\n",
    "# generate the responses\n",
    "results = await gen_data(dataset)\n",
    "\n",
    "# print(f\"Avg. Time to First Token: {sum([r['time_to_first_token'] for r in results]) / len(results):.2f} seconds\")\n",
    "# print(f\"Avg. Inter-Token Latency: {sum([r['inter_token_latency'] for r in results]) / len(results):.2f} ms/token\")\n",
    "# print(f\"Avg. Output Token Throughput: {sum([r['output_token_throughput'] for r in results]) / len(results):.2f} tokens/sec\")\n",
    "# print(f\"Avg. Generated Tokens: {sum([r['generated_tokens'] for r in results]) / len(results):.2f} tokens\")\n",
    "\n",
    "# Calculate p50 (median) for each metric\n",
    "p50_ttft = median([r[\"time_to_first_token\"] for r in results if r[\"time_to_first_token\"] is not None])\n",
    "p50_itl = median([r[\"inter_token_latency\"] for r in results if r[\"inter_token_latency\"] is not None])\n",
    "p50_throughput = median([r[\"output_token_throughput\"] for r in results if r[\"output_token_throughput\"] is not None]) \n",
    "p50_generated_tokens = median([r[\"generated_tokens\"] for r in results if r[\"generated_tokens\"] is not None])\n",
    "p50_input_tokens = median([r[\"input_tokens\"] for r in results if r[\"input_tokens\"] is not None])\n",
    "\n",
    "# Print the p50 values\n",
    "print(f\"concurrency: {concurrency}\")\n",
    "print(f\"p50 Time to First Token: {p50_ttft:.2f} seconds\")\n",
    "print(f\"p50 Inter-Token Latency: {p50_itl:.2f} ms/token\")\n",
    "print(f\"p50 Output Token Throughput: {p50_throughput:.2f} tokens/second/user\")\n",
    "print(f\"p50 Input Tokens: {p50_input_tokens:.2f} tokens\")\n",
    "print(f\"p50 Generated Tokens: {p50_generated_tokens:.2f} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TGI 2.3.1\n",
    "\n",
    "```bash\n",
    "concurrency: 4\n",
    "p50 Time to First Token: 0.36 seconds\n",
    "p50 Inter-Token Latency: 45.13 ms/token\n",
    "p50 Output Token Throughput: 22.25 tokens/second/user\n",
    "p50 Input Tokens: 715.00 tokens\n",
    "p50 Generated Tokens: 250.00 tokens\n",
    "```\n",
    "\n",
    "orca mit prefix caching 2 runs\n",
    "```bash\n",
    "concurrency: 4\n",
    "p50 Time to First Token: 0.07 seconds\n",
    "p50 Inter-Token Latency: 41.96 ms/token\n",
    "p50 Output Token Throughput: 24.03 tokens/second/user\n",
    "p50 Input Tokens: 591.00 tokens\n",
    "p50 Generated Tokens: 120.00 tokens\n",
    "```\n",
    "\n",
    "# TGI 2.2.0 \n",
    "```bash\n",
    "concurrency: 4\n",
    "p50 Time to First Token: 0.39 seconds\n",
    "p50 Inter-Token Latency: 45.58 ms/token\n",
    "p50 Output Token Throughput: 22.03 tokens/second/user\n",
    "p50 Input Tokens: 715.00 tokens\n",
    "p50 Generated Tokens: 250.00 tokens\n",
    "```\n",
    "\n",
    "# vLLM latest\n",
    "```bash\n",
    "concurrency: 4\n",
    "p50 Time to First Token: 0.38 seconds\n",
    "p50 Inter-Token Latency: 44.82 ms/token\n",
    "p50 Output Token Throughput: 22.40 tokens/second/user\n",
    "p50 Input Tokens: 715.00 tokens\n",
    "p50 Generated Tokens: 251.00 tokens\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text-to-SQL Dataset\n",
    "\n",
    "base:\n",
    "```\n",
    "Avg. Time to First Token: 0.08 seconds  \n",
    "Avg. Output Token Throughput: 16.31 tokens/sec  \n",
    "```\n",
    "\n",
    "medusa\n",
    "```\n",
    "Avg. Time to First Token: 0.08 seconds\n",
    "Avg. Output Token Throughput: 18.43 tokens/sec\n",
    "```\n",
    "\n",
    "## Open Orca Medusa Mistral\n",
    "\n",
    "base:\n",
    "```\n",
    "Avg. Time to First Token: 0.11 seconds  \n",
    "Avg. Output Token Throughput: 17.08 tokens/sec  \n",
    "```\n",
    "\n",
    "medusa\n",
    "```\n",
    "Avg. Time to First Token: 0.12 seconds\n",
    "Avg. Output Token Throughput: 22.01 tokens/sec\n",
    "```\n",
    "\n",
    "## Open Orca Llama 3\n",
    "\n",
    "base\n",
    "```\n",
    "Avg. Time to First Token: 0.05 seconds\n",
    "Avg. Inter-Token Latency: 21.82 ms/token\n",
    "Avg. Output Token Throughput: 44.26 tokens/sec\n",
    "```\n",
    "\n",
    "medusa\n",
    "```\n",
    "Avg. Time to First Token: 0.05 seconds\n",
    "Avg. Inter-Token Latency: 22.12 ms/token\n",
    "Avg. Output Token Throughput: 43.76 tokens/sec\n",
    "``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install guidellm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GuideLLM will use the OpenAI API and run through our test dataset 2,500 queries with different concurrent requests. \n",
    "\n",
    "```bash\n",
    "guidellm \\\n",
    "  --target \"http://localhost:8080/v1\" \\\n",
    "  --model meta-llama/Meta-Llama-3.1-8B-Instruct \\\n",
    "  --data philschmid/open-orca-250-guidellm \\\n",
    "  --data-type transformers \\\n",
    "  --rate-type constant \\\n",
    "  --rate 1 --rate 2 --rate 4 --rate 8 --rate 16 --rate 32 --rate 64 \\\n",
    "  --max-seconds 120 \\\n",
    "  --output-path benchmark_base.json\n",
    "```\n",
    "\n",
    "Running Llama 3.1 8b with Medusa heads on a single GPU we get the following results: \n",
    "\n",
    "```                                                                                                     \n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“ \n",
    "â”ƒ Benchmark                  â”ƒ Requests per Second â”ƒ Request Latency â”ƒ Time to First Token â”ƒ Inter Token Latency â”ƒ Output Token Throughput â”ƒ \n",
    "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”© \n",
    "â”‚ asynchronous@1.00 req/sec  â”‚ 0.94 req/sec        â”‚ 7.34 sec        â”‚ 452.72 ms           â”‚ 68.03 ms            â”‚ 94.99 tokens/sec        â”‚ \n",
    "â”‚ asynchronous@2.00 req/sec  â”‚ 0.54 req/sec        â”‚ 14.06 sec       â”‚ 3934.82 ms          â”‚ 65.29 ms            â”‚ 83.25 tokens/sec        â”‚ \n",
    "â”‚ asynchronous@4.00 req/sec  â”‚ 1.19 req/sec        â”‚ 33.81 sec       â”‚ 26663.24 ms         â”‚ 68.54 ms            â”‚ 123.47 tokens/sec       â”‚ \n",
    "â”‚ asynchronous@8.00 req/sec  â”‚ 1.22 req/sec        â”‚ 49.76 sec       â”‚ 41699.83 ms         â”‚ 68.86 ms            â”‚ 142.06 tokens/sec       â”‚ \n",
    "â”‚ asynchronous@16.00 req/sec â”‚ 1.22 req/sec        â”‚ 50.24 sec       â”‚ 41898.69 ms         â”‚ 69.09 ms            â”‚ 147.20 tokens/sec       â”‚   \n",
    "â”‚ asynchronous@32.00 req/sec â”‚ 0.89 req/sec        â”‚ 64.67 sec       â”‚ 59062.20 ms         â”‚ 46.43 ms            â”‚ 106.99 tokens/sec       â”‚ \n",
    "â”‚ asynchronous@64.00 req/sec â”‚ 0.92 req/sec        â”‚ 49.96 sec       â”‚ 39648.46 ms         â”‚ 69.34 ms            â”‚ 136.81 tokens/sec       â”‚ \n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   \n",
    "llama \n",
    "â”‚ â”‚ â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“ â”‚ â”‚\n",
    "â”‚ â”‚ â”ƒ              â”ƒ Requests per â”ƒ Request      â”ƒ Time to      â”ƒ Inter Token  â”ƒ Output Token â”ƒ â”‚ â”‚\n",
    "â”‚ â”‚ â”ƒ Benchmark    â”ƒ Second       â”ƒ Latency      â”ƒ First Token  â”ƒ Latency      â”ƒ Throughput   â”ƒ â”‚ â”‚\n",
    "â”‚ â”‚ â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”© â”‚ â”‚\n",
    "â”‚ â”‚ â”‚ asynchronouâ€¦ â”‚ 0.98 req/sec â”‚ 4.23 sec     â”‚ 53.15 ms     â”‚ 25.27 ms     â”‚ 161.42       â”‚ â”‚ â”‚\n",
    "â”‚ â”‚ â”‚ req/sec      â”‚              â”‚              â”‚              â”‚              â”‚ tokens/sec   â”‚ â”‚ â”‚\n",
    "â”‚ â”‚ â”‚ asynchronouâ€¦ â”‚ 1.21 req/sec â”‚ 15.45 sec    â”‚ 5803.01 ms   â”‚ 32.03 ms     â”‚ 364.71       â”‚ â”‚ â”‚\n",
    "â”‚ â”‚ â”‚ req/sec      â”‚              â”‚              â”‚              â”‚              â”‚ tokens/sec   â”‚ â”‚ â”‚\n",
    "â”‚ â”‚ â”‚ asynchronouâ€¦ â”‚ 1.35 req/sec â”‚ 7.02 sec     â”‚ 2633.77 ms   â”‚ 26.28 ms     â”‚ 225.47       â”‚ â”‚ â”‚\n",
    "â”‚ â”‚ â”‚ req/sec      â”‚              â”‚              â”‚              â”‚              â”‚ tokens/sec   â”‚ â”‚ â”‚\n",
    "â”‚ â”‚ â”‚ asynchronouâ€¦ â”‚ 2.17 req/sec â”‚ 22.94 sec    â”‚ 15782.63 ms  â”‚ 35.00 ms     â”‚ 442.87       â”‚ â”‚ â”‚\n",
    "â”‚ â”‚ â”‚ req/sec      â”‚              â”‚              â”‚              â”‚              â”‚ tokens/sec   â”‚ â”‚ â”‚\n",
    "â”‚ â”‚ â”‚ asynchronouâ€¦ â”‚ 2.39 req/sec â”‚ 39.75 sec    â”‚ 35062.38 ms  â”‚ 34.10 ms     â”‚ 328.25       â”‚ â”‚ â”‚\n",
    "â”‚ â”‚ â”‚ req/sec      â”‚              â”‚              â”‚              â”‚              â”‚ tokens/sec   â”‚ â”‚ â”‚\n",
    "â”‚ â”‚ â”‚ asynchronouâ€¦ â”‚ 2.41 req/sec â”‚ 15.14 sec    â”‚ 9682.72 ms   â”‚ 31.59 ms     â”‚ 416.41       â”‚ â”‚ â”‚\n",
    "â”‚ â”‚ â”‚ req/sec      â”‚              â”‚              â”‚              â”‚              â”‚ tokens/sec   â”‚ â”‚ â”‚\n",
    "â”‚ â”‚ â”‚ asynchronouâ€¦ â”‚ 2.55 req/sec â”‚ 26.56 sec    â”‚ 18058.08 ms  â”‚ 36.50 ms     â”‚ 593.67       â”‚ â”‚ â”‚\n",
    "â”‚ â”‚ â”‚ req/sec      â”‚              â”‚              â”‚              â”‚              â”‚ tokens/sec   â”‚ â”‚ â”‚\n",
    "â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚\n",
    "```\n",
    "\n",
    "Now, lets compare this to our base model without Medusa. There for we use the same `docker run` command but replace our Model id. \n",
    "\n",
    "```Bash\n",
    "CUDA_VISIBLE_DEVICES=0 docker run --gpus all -ti --shm-size 1g --ipc=host --rm -p 8080:80 \\\n",
    "  -e MODEL_ID=philschmid/llama-3-1-8b-instruct-medusa \\\n",
    "  -e NUM_SHARD=1 \\\n",
    "  -e MAX_INPUT_TOKENS=4096 \\\n",
    "  -e MAX_TOTAL_TOKENS=6000 \\\n",
    "  -e HF_TOKEN=$(cat ~/.cache/huggingface/token) \\\n",
    "  ghcr.io/huggingface/text-generation-inference:sha-b70ae09\n",
    "```\n",
    "\n",
    "And then rerun our benchmark, make sure to change the output path to `benchmark_baseline.json`. \n",
    "\n",
    "``` \n",
    "â”‚ â”‚ â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“ â”‚ â”‚\n",
    "â”‚ â”‚ â”ƒ              â”ƒ Requests per â”ƒ Request      â”ƒ Time to      â”ƒ Inter Token  â”ƒ Output Token â”ƒ â”‚ â”‚\n",
    "â”‚ â”‚ â”ƒ Benchmark    â”ƒ Second       â”ƒ Latency      â”ƒ First Token  â”ƒ Latency      â”ƒ Throughput   â”ƒ â”‚ â”‚\n",
    "â”‚ â”‚ â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”© â”‚ â”‚\n",
    "â”‚ â”‚ â”‚ asynchronouâ€¦ â”‚ 0.95 req/sec â”‚ 4.62 sec     â”‚ 55.36 ms     â”‚ 28.74 ms     â”‚ 151.05       â”‚ â”‚ â”‚\n",
    "â”‚ â”‚ â”‚ req/sec      â”‚              â”‚              â”‚              â”‚              â”‚ tokens/sec   â”‚ â”‚ â”‚\n",
    "â”‚ â”‚ â”‚ asynchronouâ€¦ â”‚ 1.63 req/sec â”‚ 14.91 sec    â”‚ 8740.49 ms   â”‚ 24.01 ms     â”‚ 416.56       â”‚ â”‚ â”‚\n",
    "â”‚ â”‚ â”‚ req/sec      â”‚              â”‚              â”‚              â”‚              â”‚ tokens/sec   â”‚ â”‚ â”‚\n",
    "â”‚ â”‚ â”‚ asynchronouâ€¦ â”‚ 1.84 req/sec â”‚ 11.89 sec    â”‚ 8128.82 ms   â”‚ 28.95 ms     â”‚ 239.15       â”‚ â”‚ â”‚\n",
    "â”‚ â”‚ â”‚ req/sec      â”‚              â”‚              â”‚              â”‚              â”‚ tokens/sec   â”‚ â”‚ â”‚\n",
    "â”‚ â”‚ â”‚ asynchronouâ€¦ â”‚ 1.87 req/sec â”‚ 17.96 sec    â”‚ 15337.43 ms  â”‚ 28.47 ms     â”‚ 171.53       â”‚ â”‚ â”‚\n",
    "â”‚ â”‚ â”‚ req/sec      â”‚              â”‚              â”‚              â”‚              â”‚ tokens/sec   â”‚ â”‚ â”‚\n",
    "â”‚ â”‚ â”‚ asynchronouâ€¦ â”‚ 2.08 req/sec â”‚ 15.62 sec    â”‚ 11674.92 ms  â”‚ 21.92 ms     â”‚ 370.78       â”‚ â”‚ â”‚\n",
    "â”‚ â”‚ â”‚ req/sec      â”‚              â”‚              â”‚              â”‚              â”‚ tokens/sec   â”‚ â”‚ â”‚\n",
    "â”‚ â”‚ â”‚ asynchronouâ€¦ â”‚ 2.68 req/sec â”‚ 22.02 sec    â”‚ 18314.22 ms  â”‚ 27.13 ms     â”‚ 364.16       â”‚ â”‚ â”‚\n",
    "â”‚ â”‚ â”‚ req/sec      â”‚              â”‚              â”‚              â”‚              â”‚ tokens/sec   â”‚ â”‚ â”‚\n",
    "â”‚ â”‚ â”‚ asynchronouâ€¦ â”‚ 3.21 req/sec â”‚ 20.85 sec    â”‚ 16561.06 ms  â”‚ 29.53 ms     â”‚ 465.71       â”‚ â”‚ â”‚\n",
    "â”‚ â”‚ â”‚ req/sec      â”‚              â”‚              â”‚              â”‚              â”‚ tokens/sec   â”‚ â”‚ â”‚\n",
    "â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lm_eval --model vllm \\\n",
    "  --tasks gsm8k_cot_llama \\\n",
    "  --model_args pretrained=meta-llama/Meta-Llama-3.1-8B-Instruct,tensor_parallel_size=1,dtype=auto,gpu_memory_utilization=0.8,data_parallel_size=4  \\\n",
    "  --apply_chat_template \\\n",
    "  --batch_size auto \\\n",
    "  --fewshot_as_multiturn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Harness with OpenAI APIs\n",
    "\n",
    "Evaluate `Meta-Llama-3.1-8B-Instruct` on the `gsm8k_cot_llama` task using the OpenAI API. The [expected performance for GMS8K is around ~0.84](https://ai.meta.com/blog/meta-llama-3-1/). [API Docs](https://github.com/EleutherAI/lm-evaluation-harness/blob/88ea85b4e54d0554e6051da71e30bf955a614954/docs/API_guide.md?plain=1#L29)\n",
    "\n",
    "Installation:\n",
    "```\n",
    "pip install lm_eval[ifeval, hf_transfer] openai\n",
    "```\n",
    "\n",
    "Tasks: \n",
    "* [gms8k](https://github.com/EleutherAI/lm-evaluation-harness/tree/88ea85b4e54d0554e6051da71e30bf955a614954/lm_eval/tasks/gsm8k) \n",
    "* [Asdiv](https://github.com/EleutherAI/lm-evaluation-harness/blob/88ea85b4e54d0554e6051da71e30bf955a614954/lm_eval/tasks/asdiv/README.md)\n",
    "* [ifeval](https://github.com/EleutherAI/lm-evaluation-harness/blob/88ea85b4e54d0554e6051da71e30bf955a614954/lm_eval/tasks/ifeval/README.md)\n",
    "\n",
    "_Note: Use this task with `--fewshot_as_multiturn` and `--apply_chat_template`` to run correctly with Llama Instruct models._\n",
    "\n",
    "  --tasks gsm8k_cot_llama,asdiv_cot_llama,ifeval \\\n",
    "```\n",
    "lm_eval --model local-chat-completions \\\n",
    "  --tasks ifeval \\\n",
    "  --model_args model=meta-llama/Meta-Llama-3.1-8B-Instruct,base_url=http://localhost:8000/v1/chat/completions,num_concurrent=32,max_retries=3,tokenized_requests=False \\\n",
    "  --apply_chat_template \\\n",
    "  --fewshot_as_multiturn\n",
    "```\n",
    "\n",
    "### TGI (2.2.0 sha-b70ae09)\n",
    "\n",
    "Run command\n",
    "``` \n",
    "CUDA_VISIBLE_DEVICES=0 docker run --gpus all -ti --shm-size 1g --ipc=host --rm -p 8000:80 \\\n",
    "  -e MODEL_ID=meta-llama/Meta-Llama-3.1-8B-Instruct \\\n",
    "  -e NUM_SHARD=1 \\\n",
    "  -e MAX_INPUT_TOKENS=10000 \\\n",
    "  -e MAX_TOTAL_TOKENS=12444 \\\n",
    "  -e HF_TOKEN=$(cat ~/.cache/huggingface/token) \\\n",
    "  --entrypoint /bin/bash \\\n",
    "  ghcr.io/huggingface/text-generation-inference:sha-b70ae09\n",
    "```\n",
    "\n",
    "Result: \n",
    "| Tasks           | Version | Filter           | n-shot | Metric                  |   | Value  |   | Stderr |\n",
    "|-----------------|---------|------------------|--------|-------------------------|---|--------|---|--------|\n",
    "| gsm8k_cot_llama | 3       | flexible-extract | 8      | exact_match             | â†‘ | 0.2206 | Â± | 0.0114 |\n",
    "|                 |         | strict-match     | 8      | exact_match             | â†‘ | 0.2153 | Â± | 0.0113 |\n",
    "| asdiv_cot_llama | 1       | flexible-extract | 8      | exact_match             | â†‘ | 0.1961 | Â± | 0.0083 |\n",
    "|                 |         | strict-match     | 8      | exact_match             | â†‘ | 0.1931 | Â± | 0.0082 |\n",
    "| ifeval          | 4       | none             | 0      | inst_level_loose_acc    | â†‘ | 0.6439 | Â± | N/A    |\n",
    "|                 |         | none             | 0      | inst_level_strict_acc   | â†‘ | 0.6295 | Â± | N/A    |\n",
    "|                 |         | none             | 0      | prompt_level_loose_acc  | â†‘ | 0.5508 | Â± | 0.0214 |\n",
    "|                 |         | none             | 0      | prompt_level_strict_acc | â†‘ | 0.5342 | Â± | 0.0215 |\n",
    "\n",
    "Runtime: \n",
    "* IFEval: 3:24 min\n",
    "* GSM8K: 06:08 min\n",
    "* Asdiv: 10:27 min\n",
    "\n",
    "## TGI (sha-ce85efa)\n",
    "\n",
    "\n",
    "Run command\n",
    "``` \n",
    "docker run --gpus all -ti --shm-size 1g --ipc=host --rm -p 8000:80 \\\n",
    "  -e MODEL_ID=meta-llama/Meta-Llama-3.1-8B-Instruct \\\n",
    "  -e NUM_SHARD=4 \\\n",
    "  -e MAX_INPUT_TOKENS=10000 \\\n",
    "  -e MAX_TOTAL_TOKENS=12444 \\\n",
    "  -e HF_TOKEN=$(cat ~/.cache/huggingface/token) \\\n",
    "  ghcr.io/huggingface/text-generation-inference:2.2.0\n",
    "```\n",
    "\n",
    "\n",
    "Result: \n",
    "|     Tasks     |Version|     Filter     |n-shot|        Metric         |   |Value |   |Stderr|\n",
    "|---------------|------:|----------------|-----:|-----------------------|---|-----:|---|------|\n",
    "|asdiv_cot_llama|      1|flexible-extract|     8|exact_match            |â†‘  |0.8243|Â±  |0.0079|\n",
    "|               |       |strict-match    |     8|exact_match            |â†‘  |0.8213|Â±  |0.0080|\n",
    "|gsm8k_cot_llama|      3|flexible-extract|     8|exact_match            |â†‘  |0.8537|Â±  |0.0097|\n",
    "|               |       |strict-match    |     8|exact_match            |â†‘  |0.8506|Â±  |0.0098|\n",
    "|ifeval         |      4|none            |     0|inst_level_loose_acc   |â†‘  |0.8501|Â±  |   N/A|\n",
    "|               |       |none            |     0|inst_level_strict_acc  |â†‘  |0.8189|Â±  |   N/A|\n",
    "|               |       |none            |     0|prompt_level_loose_acc |â†‘  |0.7837|Â±  |0.0177|\n",
    "|               |       |none            |     0|prompt_level_strict_acc|â†‘  |0.7412|Â±  |0.0188|\n",
    "\n",
    "\n",
    "Runtime: 17:54 min\n",
    "\n",
    "\n",
    "### vLLM (0.6.1)\n",
    "\n",
    "Run command\n",
    "```\n",
    "CUDA_VISIBLE_DEVICES=0 docker run --gpus all -ti --shm-size 1g --ipc=host --rm -p 8000:8000 \\\n",
    "    -e HUGGING_FACE_HUB_TOKEN=$(cat ~/.cache/huggingface/token) \\\n",
    "    vllm/vllm-openai:v0.6.1 \\\n",
    "    --model meta-llama/Meta-Llama-3.1-8B-Instruct --dtype auto\n",
    "```\n",
    "\n",
    "Result:\n",
    "|     Tasks     |Version|     Filter     |n-shot|        Metric         |   |Value |   |Stderr|\n",
    "|---------------|------:|----------------|-----:|-----------------------|---|-----:|---|------|\n",
    "|asdiv_cot_llama|      1|flexible-extract|     8|exact_match            |â†‘  |0.8221|Â±  |0.0080|\n",
    "|               |       |strict-match    |     8|exact_match            |â†‘  |0.8178|Â±  |0.0080|\n",
    "|gsm8k_cot_llama|      3|flexible-extract|     8|exact_match            |â†‘  |0.8529|Â±  |0.0098|\n",
    "|               |       |strict-match    |     8|exact_match            |â†‘  |0.8431|Â±  |0.0100|\n",
    "|ifeval         |      4|none            |     0|inst_level_loose_acc   |â†‘  |0.8561|Â±  |   N/A|\n",
    "|               |       |none            |     0|inst_level_strict_acc  |â†‘  |0.8201|Â±  |   N/A|\n",
    "|               |       |none            |     0|prompt_level_loose_acc |â†‘  |0.7967|Â±  |0.0173|\n",
    "|               |       |none            |     0|prompt_level_strict_acc|â†‘  |0.7468|Â±  |0.0187|\n",
    "\n",
    "Runtime: \n",
    "* IFEval: 3:35 min\n",
    "* GSM8K: 04:04 min\n",
    "* Asdiv: 06:05 min\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
